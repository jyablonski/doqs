---
title: Resetting AWS Account
description: A guide in my new Starlight docs site.
lastUpdated: 2025-05-23
---

import { Image } from 'astro:assets';
import { Steps } from '@astrojs/starlight/components';
import route_53 from "../../../assets/route53.png";

This doc walks through how to reset the AWS Account used for the NBA Project.

## Runbook Steps

<Steps>

1. Run Part 1 of the SQL Script (code below) to save *all* source data from
   1. `nba_source` Schema Tables
   2. `ml_models` Schema Tables
   3. `public` Schema Tables
   - **NOTE** the dbt tables can always be refreshed as long as the source data is extracted, so those aren't pulled here
2. Run `terraform destroy` to delete infrastructure in the current AWS Account
3. Run `aws-nuke -c aws_nuke.yml --no-dry-run` to delete all non-Terraform resources in the current AWS Account
   1. Make sure all fkn s3 buckets are deleted that was a bitch last time
      1. Either run `aws s3 rm s3://example-bucket --recursive` to remove all bucket contents, or manually press `Empty` on each bucket.
4. Create new `example@gmail.com` email
5. Create new AWS Account w/ new email
6. Add Root MFA onto new AWS Account
7. Go to `Account` and enable IAM Policy for Billing Acces
8. Create sign-in IAM User and grant it `AdministratorAccess` and `Billing` IAM Policies
9. Add MFA onto sign-in user
10. Create Access / Secret Keys for sign-in user
11. Create `terraform-user` and grant `AdministratorAccess` Policy
12. Create Access / Secret Keys for Terraform User
13. Update Keys in Terraform Repo & Terraform Cloud
14. Run `terraform apply` to build all infrastructure in New Account (1st run)
    - **NOTE** This will likely fail on ECS Services as the Docker Images are missing. They'll be added later
15. Update `jyablonski.dev` Route53 Record to point to provided Google Domain DNS Endpoints
    - <Image src={route_53} alt="AWS Route 53 Logo" loading="lazy" decoding="async" class="w-full h-auto" />
16. Add `jyablonski9@gmail.com` Email Verified Identity in SES manually & accept email in inbox afterwards
    - [AWS SES Link](https://us-east-1.console.aws.amazon.com/ses/home?region=us-east-1#/identities)
17. Run Liquibase Migrations to build source tables
18. Run Part 2 of the SQL Script (code below) to store *all* source data into the new RDS Database
19. Update `IAM ROLE` GitHub Actions Secret on the following Repositories:
    1.  [nba_elt_ingestion](https://github.com/jyablonski/nba_elt_ingestion)
    2.  [aws_terraform](https://github.com/jyablonski/aws_terraform)
    3.  [nba_elt_dbt](https://github.com/jyablonski/nba_elt_dbt)
    4.  [nba_elt_rest_api](https://github.com/jyablonski/nba_elt_rest_api)
    5.  [nba_elt_mlflow](https://github.com/jyablonski/nba_elt_mlflow)
    6.  [jyablonski_liquibase](https://github.com/jyablonski/jyablonski_liquibase)
20. Run all CI / CD Pipelines to build & push the Docker Images for each service to ECR
21. Run `terraform apply` to build any remaining infrastructure in New Account (2nd run)
22. Profit

</Steps>

## Troubleshooting

Process took 5 hours in August 2023 - ran into the following issues:
- AWS changed default S3 Permissions between August 2022 and August 2023.  A lot of my buckets didnt work in August 2023 when i tried to create them, something related to ACLs.  This took probably 2 extra hours of bs to figure out and finally get it working.
- Various S3 Buckets in old account didnt get deleted before I deactivated the account; had to rename these buckets in the new account.
- Backfilling the `id` serial tables that the REST API uses was a bitch without Migrations
- Like half the Repos still had Access/Secret Key Auth for CI / CD instead of the IAM Role so this took like an extra hour to build that on the fly so it's doing it the right way with OIDC
- Without the S3 permissions or bucket renaming issues, this process should take < 1 hour

## SQL Script Code

<details>
<summary>SQL Save & Reload Script</summary>


``` sql
from datetime import datetime
import os

from jyablonski_common_modules.sql import sql_connection, write_to_sql
import pandas as pd

engine = sql_connection(
    database=os.environ.get("RDS_DB"),
    schema="nba_source",
    user=os.environ.get("RDS_USER"),
    pw=os.environ.get("RDS_PW"),
    host=os.environ.get("IP"),
)

ml_models_tables = [
    "ml_game_predictions",
]

nba_prod_tables = [
    "feature_flags",
    "feature_flags_audit",
    "incidents",
    "incidents_audit",
    "rest_api_users",
    "rest_api_users_audit",
    "user_predictions",
    "user_predictions_audit",
]

nba_source_tables = [
    "aws_adv_stats_source",
    "aws_boxscores_source",
    "aws_contracts_source",
    "aws_injury_data_source",
    "aws_odds_source",
    "aws_opp_stats_source",
    "aws_pbp_data_source",
    "aws_player_attributes_source",
    "aws_preseason_odds_source",
    "aws_reddit_comment_data_source",
    "aws_reddit_data_source",
    "aws_schedule_source",
    "aws_shooting_stats_source",
    "aws_stats_source",
    "aws_team_attributes_source",
    "aws_transactions_source",
    "aws_twitter_data_source",
    "aws_twitter_tweepy_data_source",
    "aws_twitter_tweets_source",
    "inactive_dates",
    "staging_seed_player_attributes",
    "staging_seed_team_attributes",
    "staging_seed_top_players",
]

public_tables = [
    "rest_api_users",
    "user_predictions",
]


-- PART 1
-- RUN THIS BEFORE old AWS Account is destroyed to save the data
def store_sql_table(connection, table: str, schema: str):
    todays_date = datetime.now().date()

    try:
        df = pd.read_sql(f"select * from {schema}.{table};", con=connection)
        print(f"Queried {len(df)} Records from {schema}.{table}")

        df.to_parquet(f"sql/tables/{schema}/{table}-{todays_date}.parquet")
        print(
            f"Wrote {schema}.{table} to tables/{schema}/{table}-{todays_date}.parquet"
        )

        pass
    except BaseException as e:
        print(f"Error Occurred while Reading {schema}.{table}, {e}")
        raise e

-- PART 2
-- RUN THIS AFTER new AWS Account is created & RDS Database is available to reload the data
with engine.connect() as connection:
    for table in ml_models_tables:
        store_sql_table(connection=connection, table=table, schema="ml_models")

    for table in nba_prod_tables:
        store_sql_table(connection=connection, table=table, schema="nba_prod")

    for table in nba_source_tables:
        store_sql_table(connection=connection, table=table, schema="nba_source")

    for table in public_tables:
        store_sql_table(connection=connection, table=table, schema="nba_prod")
```

</details>
